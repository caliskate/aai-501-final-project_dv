{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change notes 11/22/24:\n",
    "\n",
    "# moved data.yaml file to dataset folder per YOLO documentation\n",
    "# dataset needs to be in same folder as notebook\n",
    "# updated yaml.data to 10 classes, due to dataset error for only subset to 7 or 8\n",
    "# added label dir splitting process\n",
    "# added markdowns\n",
    "# prevented raw_images and raw_labels from being cleared\n",
    "# trained model from labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP: \n",
    "<br> fix yaml location pull (yaml needs to be in same folder as images)\n",
    "<br>any further fine tuning on hyperparams\n",
    "<br>add validation plots,\n",
    "<br>initial and predicted image subplots *from custom dataset only*\n",
    "<br>live video feed,\n",
    "<br>dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os # file \n",
    "import shutil\n",
    "import cv2 # opencv for images\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ultralytics import YOLO #for obeject detection\n",
    "import albumentations as A # for image augmentation\n",
    "from albumentations.pytorch import ToTensorV2 # for image formating\n",
    "# from tqdm import tqdm  # to show processing progress\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear the \"train\", \"test\", \"valid\" folders from base dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated directory: ./datasets/images/train\n",
      "Recreated directory: ./datasets/images/valid\n",
      "Recreated directory: ./datasets/images/test\n",
      "Recreated directory: ./datasets/labels/train\n",
      "Recreated directory: ./datasets/labels/valid\n",
      "Recreated directory: ./datasets/labels/test\n",
      "Directories reset and ready for use.\n"
     ]
    }
   ],
   "source": [
    "# create direcories to organize images and cleanup for a new to avoid duplicate images \n",
    "def reset_directories(root_dir, directories):\n",
    "    \"\"\"\n",
    "    Check if the specified directories exist. If they do, delete them and recreate them.\n",
    "    Ensures the directories are clean before use.\n",
    "\n",
    "    Parameters:\n",
    "        directories (list): List of directories to reset.\n",
    "    \"\"\"\n",
    "    for path in directories:\n",
    "        dir_path = root_dir+path\n",
    "        if os.path.exists(dir_path):\n",
    "            # delete the directory and all its contents\n",
    "            try:\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Deleted existing directory: {dir_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {dir_path}. Reason: {e}\")\n",
    "        \n",
    "        # Recreate the directory\n",
    "        try:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            print(f\"Recreated directory: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create directory {dir_path}. Reason: {e}\")\n",
    "# base dir\n",
    "dataset_base_dir = \"./datasets\"\n",
    "\n",
    "# Define directories to reset\n",
    "directories_to_reset = [\n",
    "    \"/images/train\",\n",
    "    \"/images/valid\",\n",
    "    \"/images/test\",\n",
    "    \"/labels/train\",\n",
    "    \"/labels/valid\",\n",
    "    \"/labels/test\"\n",
    "]\n",
    "\n",
    "# Reset directories\n",
    "reset_directories(dataset_base_dir, directories_to_reset)\n",
    "\n",
    "print(\"Directories reset and ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split files from \"raw_image\" and \"raw_label\" folders\n",
    "#### (Note: \"./datasets/raw_images\" and \"./datasets/raw_labels\" folders must already contain all images and all labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# source directory containing all raw images and labels\n",
    "source_image_dir = \"./datasets/raw_images\"\n",
    "source_label_dir = \"./datasets/raw_labels\"\n",
    "\n",
    "# base directory for the organized dataset\n",
    "base_dir = dataset_base_dir\n",
    "\n",
    "image_dirs = {\n",
    "    \"train\": os.path.join(base_dir, \"images/train\"),\n",
    "    \"valid\": os.path.join(base_dir, \"images/valid\"),\n",
    "    \"test\": os.path.join(base_dir, \"images/test\")\n",
    "}\n",
    "label_dirs = {\n",
    "    \"train\": os.path.join(base_dir, \"labels/train\"),\n",
    "    \"valid\": os.path.join(base_dir, \"labels/valid\"),\n",
    "    \"test\": os.path.join(base_dir, \"labels/test\")\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in list(image_dirs.values()) + list(label_dirs.values()):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Extract the image files\n",
    "image_files = [f for f in os.listdir(source_image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Create DataFrame with file paths and dataset split assignments\n",
    "df = pd.DataFrame({\"filename\": image_files})\n",
    "\n",
    "# Dynamically split dataset into train, valid, and test\n",
    "train_size = int(0.7 * len(df))  # 70% for training\n",
    "valid_size = int(0.2 * len(df))  # 20% for validation\n",
    "test_size = len(df) - train_size - valid_size  # remaining 10% for testing\n",
    "\n",
    "train_labels = [\"train\"] * train_size\n",
    "valid_labels = [\"valid\"] * valid_size\n",
    "test_labels = [\"test\"] * test_size\n",
    "\n",
    "# Combine and assign to DataFrame\n",
    "df[\"split\"] = train_labels + valid_labels + test_labels\n",
    "\n",
    "# Copy files to respective folders\n",
    "for _, row in df.iterrows():\n",
    "    img_file = row[\"filename\"]\n",
    "    split = row[\"split\"]\n",
    "    \n",
    "    # Source paths\n",
    "    img_src = os.path.join(source_image_dir, img_file)\n",
    "    label_src = os.path.join(source_label_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "    \n",
    "    # Destination paths\n",
    "    img_dest = os.path.join(image_dirs[split], img_file)\n",
    "    label_dest = os.path.join(label_dirs[split], os.path.splitext(img_file)[0] + \".txt\")\n",
    "    \n",
    "    # Copy image file\n",
    "    if os.path.exists(img_src):\n",
    "        shutil.copy(img_src, img_dest)\n",
    "    \n",
    "    # Copy corresponding label file\n",
    "    if os.path.exists(label_src):\n",
    "        shutil.copy(label_src, label_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.36  Python-3.12.7 torch-2.5.1+cpu CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train57, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train57\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\User\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 755k/755k [00:00<00:00, 4.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\Documents\\GitHub\\aai-501-final-project_dv\\datasets\\labels\\test... 287 images, 5 backgroun\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\User\\Documents\\GitHub\\aai-501-final-project_dv\\datasets\\labels\\test.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\Documents\\GitHub\\aai-501-final-project_dv\\datasets\\labels\\valid... 571 images, 4 background\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\User\\Documents\\GitHub\\aai-501-final-project_dv\\datasets\\labels\\valid.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train57\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train57\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.504      3.784      1.553        184        640: 100%|██████████| 18/18 [01:36<00:00,  5.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [01:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        571       7714      0.024      0.306     0.0936     0.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.419      3.269       1.52        186        640: 100%|██████████| 18/18 [01:26<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [01:30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        571       7714     0.0191      0.324     0.0988     0.0576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G       1.43      2.632      1.522        228        640: 100%|██████████| 18/18 [01:26<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [01:27"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        571       7714      0.828     0.0902      0.184      0.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.394      2.308      1.497        188        640:  83%|████████▎ | 15/18 [01:13<00:15,  5."
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "model.train(data='data.yaml', epochs = 10, val = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YOLOv8 pre-trained model\n",
    "model = YOLO(\"../models/yolov8n.pt\")\n",
    "\n",
    "# define augmentation pipeline\n",
    "augmentation = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),  # No change to pixel values\n",
    "    ToTensorV2()  # convert to PyTorch tensor \n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['labels']))\n",
    "\n",
    "# define function to annotate and process images\n",
    "def annotate_images(df, split, image_dirs, label_dirs, confidence_threshold=0.5):\n",
    "    data = []\n",
    "    split_df = df[df[\"split\"] == split]\n",
    "    for _, row in split_df.iterrows():\n",
    "        image_file = row[\"filename\"]\n",
    "        # load the image\n",
    "        image_path = os.path.join(source_image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # perform inference using YOLOv8\n",
    "        results = model(image_path)\n",
    "\n",
    "        # extract bounding boxes and labels\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for result in results[0].boxes:\n",
    "            box = result.xywhn[0].cpu().numpy()  # Normalized x_center, y_center, width, height\n",
    "            class_id = int(result.cls[0].cpu().numpy())\n",
    "            confidence = float(result.conf[0].cpu().numpy())\n",
    "\n",
    "            # filter by confidence threshold\n",
    "            if confidence >= confidence_threshold:\n",
    "                bboxes.append(box.tolist())\n",
    "                labels.append(class_id)\n",
    "\n",
    "                # add annotation details to the df list\n",
    "                data.append({\n",
    "                    \"filename\": image_file,\n",
    "                    \"split\": split,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"x_center\": box[0],\n",
    "                    \"y_center\": box[1],\n",
    "                    \"width\": box[2],\n",
    "                    \"height\": box[3],\n",
    "                })\n",
    "\n",
    "        # apply augmentation\n",
    "        if bboxes:  # Only augment if there are bounding boxes\n",
    "            augmented = augmentation(image=image, bboxes=bboxes, labels=labels)\n",
    "            image = augmented[\"image\"]\n",
    "            bboxes = augmented[\"bboxes\"]\n",
    "            labels = augmented[\"labels\"]\n",
    "\n",
    "        # convert to numpy format for saving \n",
    "        if isinstance(image, torch.Tensor):  # If tensor, convert to numpy for opencv\n",
    "            image = image.permute(1, 2, 0).cpu().numpy() # changes the order of the tensor dimensions from (C, H, W) (Channel-Height-Width, common in PyTorch) to (H, W, C) (Height-Width-Channel, required by OpenCV and most image libraries).\n",
    "            image = (image * 255).astype(np.uint8)  # Convert to uint8 for OpenCV\n",
    "\n",
    "        # save the image to the appropriate directory\n",
    "        output_image_path = os.path.join(image_dirs[split], image_file)\n",
    "        cv2.imwrite(output_image_path, image)\n",
    "\n",
    "        # set YOLO format labels\n",
    "        label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "        label_path = os.path.join(label_dirs[split], label_file)\n",
    "\n",
    "        # create YOLO format label for the corresponding image\n",
    "        with open(label_path, \"w\") as f:\n",
    "            for bbox, class_id in zip(bboxes, labels):\n",
    "                # Write each valid detection to the file in YOLO format\n",
    "                f.write(f\"{class_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\n\")\n",
    "\n",
    "    return data\n",
    "# annotate and process images for all splits\n",
    "annotation_data = []\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    # append each list to annotation data\n",
    "    annotation_data.extend(annotate_images(df, split, image_dirs, label_dirs, confidence_threshold=0.5))\n",
    "\n",
    "\n",
    "print(\"Annotation completed and dataset organized into train, validation, and test directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df = pd.DataFrame(annotation_data)\n",
    "annotation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
