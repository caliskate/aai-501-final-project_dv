{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change notes 11/22/24:\n",
    "\n",
    "# updated yaml.data to 8 classes\n",
    "# added label dir splitting process\n",
    "# added markdowns\n",
    "# prevented raw_images and raw_labels from being cleared\n",
    "# trained model from labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP: \n",
    "<br>any further fine tuning on hyperparams\n",
    "<br>add validation plots,\n",
    "<br>initial and predicted image subplots *from custom dataset only*\n",
    "<br>live video feed,\n",
    "<br>dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os # file \n",
    "import shutil\n",
    "import cv2 # opencv for images\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ultralytics import YOLO #for obeject detection\n",
    "import albumentations as A # for image augmentation\n",
    "from albumentations.pytorch import ToTensorV2 # for image formating\n",
    "# from tqdm import tqdm  # to show processing progress\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear the \"train\", \"test\", \"valid\" folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing directory: ../datasets/images/train\n",
      "Recreated directory: ../datasets/images/train\n",
      "Deleted existing directory: ../datasets/images/valid\n",
      "Recreated directory: ../datasets/images/valid\n",
      "Deleted existing directory: ../datasets/images/test\n",
      "Recreated directory: ../datasets/images/test\n",
      "Deleted existing directory: ../datasets/labels/train\n",
      "Recreated directory: ../datasets/labels/train\n",
      "Deleted existing directory: ../datasets/labels/valid\n",
      "Recreated directory: ../datasets/labels/valid\n",
      "Deleted existing directory: ../datasets/labels/test\n",
      "Recreated directory: ../datasets/labels/test\n",
      "Directories reset and ready for use.\n"
     ]
    }
   ],
   "source": [
    "# create direcories to organize images and cleanup for a new to avoid duplicate images \n",
    "def reset_directories(root_dir, directories):\n",
    "    \"\"\"\n",
    "    Check if the specified directories exist. If they do, delete them and recreate them.\n",
    "    Ensures the directories are clean before use.\n",
    "\n",
    "    Parameters:\n",
    "        directories (list): List of directories to reset.\n",
    "    \"\"\"\n",
    "    for path in directories:\n",
    "        dir_path = root_dir+path\n",
    "        if os.path.exists(dir_path):\n",
    "            # delete the directory and all its contents\n",
    "            try:\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"Deleted existing directory: {dir_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {dir_path}. Reason: {e}\")\n",
    "        \n",
    "        # Recreate the directory\n",
    "        try:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            print(f\"Recreated directory: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create directory {dir_path}. Reason: {e}\")\n",
    "# base dir\n",
    "dataset_base_dir = \"../datasets\"\n",
    "\n",
    "# Define directories to reset\n",
    "directories_to_reset = [\n",
    "    \"/images/train\",\n",
    "    \"/images/valid\",\n",
    "    \"/images/test\",\n",
    "    \"/labels/train\",\n",
    "    \"/labels/valid\",\n",
    "    \"/labels/test\"\n",
    "]\n",
    "\n",
    "# Reset directories\n",
    "reset_directories(dataset_base_dir, directories_to_reset)\n",
    "\n",
    "print(\"Directories reset and ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split files from \"raw_image\" and \"raw_label\" folders\n",
    "#### (Note: \"../datasets/raw_images\" and \"../datasets/raw_labels\" folders must already contain all images and all labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# source directory containing all raw images and labels\n",
    "source_image_dir = \"../datasets/raw_images\"\n",
    "source_label_dir = \"../datasets/raw_labels\"\n",
    "\n",
    "# base directory for the organized dataset\n",
    "base_dir = dataset_base_dir\n",
    "\n",
    "image_dirs = {\n",
    "    \"train\": os.path.join(base_dir, \"images/train\"),\n",
    "    \"valid\": os.path.join(base_dir, \"images/valid\"),\n",
    "    \"test\": os.path.join(base_dir, \"images/test\")\n",
    "}\n",
    "label_dirs = {\n",
    "    \"train\": os.path.join(base_dir, \"labels/train\"),\n",
    "    \"valid\": os.path.join(base_dir, \"labels/valid\"),\n",
    "    \"test\": os.path.join(base_dir, \"labels/test\")\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in list(image_dirs.values()) + list(label_dirs.values()):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Extract the image files\n",
    "image_files = [f for f in os.listdir(source_image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Create DataFrame with file paths and dataset split assignments\n",
    "df = pd.DataFrame({\"filename\": image_files})\n",
    "\n",
    "# Dynamically split dataset into train, valid, and test\n",
    "train_size = int(0.7 * len(df))  # 70% for training\n",
    "valid_size = int(0.2 * len(df))  # 20% for validation\n",
    "test_size = len(df) - train_size - valid_size  # remaining 10% for testing\n",
    "\n",
    "train_labels = [\"train\"] * train_size\n",
    "valid_labels = [\"valid\"] * valid_size\n",
    "test_labels = [\"test\"] * test_size\n",
    "\n",
    "# Combine and assign to DataFrame\n",
    "df[\"split\"] = train_labels + valid_labels + test_labels\n",
    "\n",
    "# Copy files to respective folders\n",
    "for _, row in df.iterrows():\n",
    "    img_file = row[\"filename\"]\n",
    "    split = row[\"split\"]\n",
    "    \n",
    "    # Source paths\n",
    "    img_src = os.path.join(source_image_dir, img_file)\n",
    "    label_src = os.path.join(source_label_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "    \n",
    "    # Destination paths\n",
    "    img_dest = os.path.join(image_dirs[split], img_file)\n",
    "    label_dest = os.path.join(label_dirs[split], os.path.splitext(img_file)[0] + \".txt\")\n",
    "    \n",
    "    # Copy image file\n",
    "    if os.path.exists(img_src):\n",
    "        shutil.copy(img_src, img_dest)\n",
    "    \n",
    "    # Copy corresponding label file\n",
    "    if os.path.exists(label_src):\n",
    "        shutil.copy(label_src, label_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to '..\\models\\yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                            | 128k/6.25M [00:00<00:03, 1.75MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load YOLOv8 pre-trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/yolov8n.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# define augmentation pipeline\u001b[39;00m\n\u001b[0;32m      5\u001b[0m augmentation \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      6\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m      7\u001b[0m     A\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)),  \u001b[38;5;66;03m# No change to pixel values\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     ToTensorV2()  \u001b[38;5;66;03m# convert to PyTorch tensor \u001b[39;00m\n\u001b[0;32m      9\u001b[0m ], bbox_params\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mBboxParams(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolo\u001b[39m\u001b[38;5;124m'\u001b[39m, label_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:145\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:285\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    282\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:910\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    909\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 910\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    912\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:815\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight, safe_only)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_download_asset\n\u001b[0;32m    814\u001b[0m check_suffix(file\u001b[38;5;241m=\u001b[39mweight, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 815\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_download_asset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# search online if missing locally\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_modules(\n\u001b[0;32m    818\u001b[0m         modules\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    819\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.yolo.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    827\u001b[0m         },\n\u001b[0;32m    828\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\downloads.py:454\u001b[0m, in \u001b[0;36mattempt_download_asset\u001b[1;34m(file, repo, release, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m         safe_download(url\u001b[38;5;241m=\u001b[39murl, file\u001b[38;5;241m=\u001b[39mfile, min_bytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m repo \u001b[38;5;241m==\u001b[39m GITHUB_ASSETS_REPO \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m GITHUB_ASSETS_NAMES:\n\u001b[1;32m--> 454\u001b[0m     \u001b[43msafe_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdownload_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrelease\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     tag, assets \u001b[38;5;241m=\u001b[39m get_github_assets(repo, release)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\downloads.py:341\u001b[0m, in \u001b[0;36msafe_download\u001b[1;34m(url, file, dir, unzip, delete, curl, retry, min_bytes, exist_ok, progress)\u001b[0m\n\u001b[0;32m    339\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 341\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m request\u001b[38;5;241m.\u001b[39murlopen(url) \u001b[38;5;28;01mas\u001b[39;00m response, TQDM(\n\u001b[0;32m    344\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mgetheader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    345\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m         unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m    350\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:744\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[0;32m    737\u001b[0m     total\u001b[38;5;241m=\u001b[39mfile_size,\n\u001b[0;32m    738\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    741\u001b[0m     unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m    742\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m         buffer \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_DATA_CHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load YOLOv8 pre-trained model\n",
    "model = YOLO(\"../models/yolov8n.pt\")\n",
    "\n",
    "# define augmentation pipeline\n",
    "augmentation = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),  # No change to pixel values\n",
    "    ToTensorV2()  # convert to PyTorch tensor \n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['labels']))\n",
    "\n",
    "# define function to annotate and process images\n",
    "def annotate_images(df, split, image_dirs, label_dirs, confidence_threshold=0.5):\n",
    "    data = []\n",
    "    split_df = df[df[\"split\"] == split]\n",
    "    for _, row in split_df.iterrows():\n",
    "        image_file = row[\"filename\"]\n",
    "        # load the image\n",
    "        image_path = os.path.join(source_image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # perform inference using YOLOv8\n",
    "        results = model(image_path)\n",
    "\n",
    "        # extract bounding boxes and labels\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for result in results[0].boxes:\n",
    "            box = result.xywhn[0].cpu().numpy()  # Normalized x_center, y_center, width, height\n",
    "            class_id = int(result.cls[0].cpu().numpy())\n",
    "            confidence = float(result.conf[0].cpu().numpy())\n",
    "\n",
    "            # filter by confidence threshold\n",
    "            if confidence >= confidence_threshold:\n",
    "                bboxes.append(box.tolist())\n",
    "                labels.append(class_id)\n",
    "\n",
    "                # add annotation details to the df list\n",
    "                data.append({\n",
    "                    \"filename\": image_file,\n",
    "                    \"split\": split,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"x_center\": box[0],\n",
    "                    \"y_center\": box[1],\n",
    "                    \"width\": box[2],\n",
    "                    \"height\": box[3],\n",
    "                })\n",
    "\n",
    "        # apply augmentation\n",
    "        if bboxes:  # Only augment if there are bounding boxes\n",
    "            augmented = augmentation(image=image, bboxes=bboxes, labels=labels)\n",
    "            image = augmented[\"image\"]\n",
    "            bboxes = augmented[\"bboxes\"]\n",
    "            labels = augmented[\"labels\"]\n",
    "\n",
    "        # convert to numpy format for saving \n",
    "        if isinstance(image, torch.Tensor):  # If tensor, convert to numpy for opencv\n",
    "            image = image.permute(1, 2, 0).cpu().numpy() # changes the order of the tensor dimensions from (C, H, W) (Channel-Height-Width, common in PyTorch) to (H, W, C) (Height-Width-Channel, required by OpenCV and most image libraries).\n",
    "            image = (image * 255).astype(np.uint8)  # Convert to uint8 for OpenCV\n",
    "\n",
    "        # save the image to the appropriate directory\n",
    "        output_image_path = os.path.join(image_dirs[split], image_file)\n",
    "        cv2.imwrite(output_image_path, image)\n",
    "\n",
    "        # set YOLO format labels\n",
    "        label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "        label_path = os.path.join(label_dirs[split], label_file)\n",
    "\n",
    "        # create YOLO format label for the corresponding image\n",
    "        with open(label_path, \"w\") as f:\n",
    "            for bbox, class_id in zip(bboxes, labels):\n",
    "                # Write each valid detection to the file in YOLO format\n",
    "                f.write(f\"{class_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\n\")\n",
    "\n",
    "    return data\n",
    "# annotate and process images for all splits\n",
    "annotation_data = []\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    # append each list to annotation data\n",
    "    annotation_data.extend(annotate_images(df, split, image_dirs, label_dirs, confidence_threshold=0.5))\n",
    "\n",
    "\n",
    "print(\"Annotation completed and dataset organized into train, validation, and test directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>split</th>\n",
       "      <th>class_id</th>\n",
       "      <th>confidence</th>\n",
       "      <th>x_center</th>\n",
       "      <th>y_center</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_0871_MOV-39_jpg.rf.41a6a9b70c2a41fa45e3960...</td>\n",
       "      <td>train</td>\n",
       "      <td>7</td>\n",
       "      <td>0.734378</td>\n",
       "      <td>0.523683</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.118504</td>\n",
       "      <td>0.105876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.743984</td>\n",
       "      <td>0.204548</td>\n",
       "      <td>0.387022</td>\n",
       "      <td>0.275569</td>\n",
       "      <td>0.277825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.689993</td>\n",
       "      <td>0.369293</td>\n",
       "      <td>0.614387</td>\n",
       "      <td>0.054831</td>\n",
       "      <td>0.048369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.587490</td>\n",
       "      <td>0.519868</td>\n",
       "      <td>0.616981</td>\n",
       "      <td>0.059734</td>\n",
       "      <td>0.045449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.581372</td>\n",
       "      <td>0.617744</td>\n",
       "      <td>0.605309</td>\n",
       "      <td>0.067972</td>\n",
       "      <td>0.064742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  split  class_id  \\\n",
       "0  IMG_0871_MOV-39_jpg.rf.41a6a9b70c2a41fa45e3960...  train         7   \n",
       "1  004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...  train         0   \n",
       "2  004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...  train         2   \n",
       "3  004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...  train         2   \n",
       "4  004720_jpg.rf.afc486560a4004c7cfd67910af31a29c...  train         2   \n",
       "\n",
       "   confidence  x_center  y_center     width    height  \n",
       "0    0.734378  0.523683  0.663333  0.118504  0.105876  \n",
       "1    0.743984  0.204548  0.387022  0.275569  0.277825  \n",
       "2    0.689993  0.369293  0.614387  0.054831  0.048369  \n",
       "3    0.587490  0.519868  0.616981  0.059734  0.045449  \n",
       "4    0.581372  0.617744  0.605309  0.067972  0.064742  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_df = pd.DataFrame(annotation_data)\n",
    "annotation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
